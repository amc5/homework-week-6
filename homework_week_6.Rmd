---
title: "homework_week_6"
output: html_document
---
```{r}
knitr::opts_chunk$set(
    echo=TRUE,
    warning=FALSE,
    comment="##",
    prompt=TRUE,
    tidy=TRUE,
    tidy.opts=list(width.cutoff=75),
    fig.path="img/"
)
```
[1] Write a simple R function, Z.prop.test(), that can perform one- or two-sample Z-tests for proportion data, using the following guidelines.
*Your function should take the following arguments: p1 and n1 (no default) to pose as the estimated proportion and sample size (i.e., based on your sample data); p2 and n2 (both defaulting to NULL) that contain a second sample's proportion and sample size data in the event of a two-sample test; p0 (no default) as the expected value for the population proportion; and alternative (default "two.sided") and conf.level (default 0.95), to be used in the same way as in the function t.test().
*When conducting a two-sample test, it should be p1 that is tested as being smaller or larger than p2 when alternative="less" or alternative="greater", the same as in the use of x and y in the function t.test().
*The function should perform a one-sample Z-test using p1, n1, and p0 if either p2 or n2 (or both) is NULL.
*The function should contain a check for the rules of thumb we have talked about ($n * p > 5$ and $n * (1-p) >5$) to ensure the validity of assuming the normal distribution in both the one- and two-sample settings. If this is violated, the function should still complete but it should also print an appropriate warning message.
*The function should return a list containing the members Z (the test statistic), P (the appropriate p-value), and CI (the two-sided CI with respect to confidence level).
#number one
```{r}
# calculating z statistic equation
# z= Phat-P0/SE(Phat) = Phat-P0/(sqrt(pq/n))
# CI = estimate +- margin of error

z.prop.test.() <- function(p1,n1,p2=NULL, n2=NULL, p0, alternative="two.sided", conf.level=0.95) #these are the functions arguments 
  { 
  if(p2=NULL||n2=NULL) { #this is a one-sample z test
  se.p1<- sqrt((p0*1-p0)/n1)
  z<- (p1-p0)/se.p1 #this is the z test = (phat-p0/se(phat))
  p.val<- pnorm(z) #p value of test
  ci<- p1 + c(-1*((qnorm(((1-conf.level)/2)+ con.level))*se.p1), ((qnorm(((1-conf.level)/2)+ conf.level))*se.p1))
  }
    if(n1*p1<=5||n1*(1-p1)<=5){ #check for the rules of thumb- data should be normally distributed, test will still run but the below cuidado message will be printed
      warning("CUIDADO: Sample may not be normally distributed, and does not meet assumptions for a Z-test") #end of the code for the one sample z test
    }
  else{
  if(p1*n1<=5||n1*(1-p1)<=5||p2*n2<=5||n2*(1-p2<=5)) { #this tests to see if the sample size is large enough to run the Z test
  warning("CUIDADO: Sample my not be normally distributed, and does not meet assumptions for a z-test")
  }
  p.star<- (p1*n1+p2*n2)/(n1+n2) #pooled proportion of probabilities
  z<- (p1-p2-p0)/sqrt(p.star*(1-p.star)*(1/n1+1/n2)) #this is the code for the two-sample z-test (estimate +- margin of error)
  ci<-(p1-p2)+c(-1,1)*qnorm(conf.level+(1-conf.level)/2)*sqrt(p.star*(1-p.star)*(1/n1+1/n2)) #confidence interval for two sample test
  }
  if(alternative=="two.sided"){
    if (z>0){
      p.val<- 2*pnorm(z, lower.tail=FALSE)#multiplied by 2 because it is a two-sample z-test
    }
  }
    if(z<0){
      p.val<- 2*pnorm(z, lower.tail=TRUE)
    }
  if(alternative=="greater"){
    p.val<- pnorm(z, lower.tail=FALSE) #calculates the associated p value
  }
  if(alternative== "less"){
    p.val <- pnorm(z, lower.tail = TRUE) #calculates associated p value
  }
  return(list(z, p, ci))
}  
```

[2] The dataset from Kamilar and Cooper has in it a large number of variables related to life history and body size. For this exercise, the end aim is to fit a simple linear regression model to predict longevity ("MaxLongevity_m") measured in months from species' brain size ("Brain_Size_Species_Mean") measured in grams. Do the following for both longevity~brain size and log(longevity)~log(brain size).

*Fit the regression model and, using {ggplot2}, produce a scatterplot with the fitted line superimposed upon the data. Append the the fitted model equation to your plot (HINT: use the function geom_text()).
*Identify and interpret the point estimate of the slope ($\beta_1$), as well as the outcome of the test associated with the hypotheses H0: $\beta_1$ = 0; HA: $\beta_1$ ≠ 0. Also, find a 90 percent CI for the slope ($\beta_1$) parameter.
*Using your model, add lines for the 90 percent confidence and prediction interval bands on the plot and add a legend to differentiate between the lines.
*Produce a point estimate and associated 90 percent PI for the longevity of a species whose brain weight is 800 gm. Do you trust the model to predict observations accurately for this value of the explanatory variable? Why or why not?
*Looking at your two models, which do you think is better? Why?

#number two
*Fit the regression model and, using {ggplot2}, produce a scatterplot with the fitted line superimposed upon the data. Append the the fitted model equation to your plot (HINT: use the function geom_text()).
```{r}
f<-file.choose() #uploading data into R
f
d<-read.csv(f)
head(d)
library(ggplot2)

m<- lm(data=d, MaxLongevity_m~Brain_Size_Species_Mean) #creates linear regression model
m
m$coefficients
head(m$model)
brain_size<- d$Brain_Size_Species_Mean
maximum_longevity <- d$MaxLongevity_m
head(m)
summary(m) #shows summary of the model data
g <- ggplot(data=d, aes(x=brain_size, y= maximum_longevity))
g <- g + geom_point()
g <- g + geom_smooth(method = "lm", formula = y~x, color= "lawngreen")
g #I am getting a warning message with this line of code that 85 rows have been removed due to non-finite values and missing values
g <- g + geom_label(x= 250, y= 850, label= "y = 1.218x + 248.952", color="mediumvioletred") #the x and y coordinates put the equation in a specific equation, I used geom_label to put the box around the text to make it easier to read
g
#below is with log transformed data
lg_brain_size<- log(d$Brain_Size_Species_Mean)
lg_MaxLongevity<- log(d$MaxLongevity_m)
logm<- lm(data=d, lg_MaxLongevity~lg_brain_size)
logm
logg<-ggplot(data=d, aes(x=lg_brain_size, y= lg_MaxLongevity))
logg<- logg +geom_point()
logg <- logg + geom_smooth(method = "lm" , formula = y~x, color = "magenta")
logg <- logg + geom_label(x= 3, y= 5, label = "y = 0.2341x + 2.4790", color = "forestgreen")
logg
```
*Identify and interpret the point estimate of the slope ($\beta_1$), as well as the outcome of the test associated with the hypotheses H0: $\beta_1$ = 0; HA: $\beta_1$ ≠ 0. Also, find a 90 percent CI for the slope ($\beta_1$) parameter.
```{r}
plot(data=d, MaxLongevity_m~Brain_Size_Species_Mean)
head(d)
#this is me trying to do it by hand, and me failing at it, I don't know why I am getting NA back for these functions
as.numeric(d$MaxLongevity_m)
as.numeric(d$Brain_Size_Species_Mean) #trying to not get NA in below equations by making data numeric, but didn't work.
beta1 <- cor(d$MaxLongevity_m,d$Brain_Size_Species_Mean)*sd(d$MaxLongevity_m)/sd(d$Brain_Size_Species_Mean)
beta1 #why does this keep returning NA
beta0 <- mean(m$maximum_longevity) - beta1*mean(m$brain_size)
beta0 #receive warning message "in mean.default argument is not numeric or logical returning NA" . keeping these lines of code in here so that I can go back later and understand
m <- lm(maximum_longevity~brain_size, data=d) #built in R function for linear model and gives you all the info estimating intercepts
m
summary(m)
##The point estimate of the slope seen as "Estimate of brain_size" is 1.2180 which means that for every gram increase in mean brain size in a species, the maximum longevity mean of the species increases by 1.218 months (Beta1=expected change in y with every 1 unit change in x)
summary(logm)
##The point estimate of the slope for the log transformed data labeled "estimate of lg_brain_size" is 0.23415.  This means that for every gram increased in the log brain size in a species, the log maximum longevity mean of the speices increases in 0.23415 months.
##The standardized t-value for the predictor variable, brain_size, is 11.06, and the p calue is 2e-16 for both the intercept and the predictor variablie. These values are the results of the test associated with the hypothesis H0:Beta1=0. According to our results, a high t value and low p value, we can reject our H0 that there is no effect of brain size on longevity. There is an effect of the covariate.

confint(m, level= 0.90)
##90% confidence interval for the slope is between 1.035571 and 1.40041.
confint(logm, level=0.90)
##90% CI for the slope of the log transformed data is between 0.2046396 and 0.2636595.
```

*Using your model, add lines for the 90 percent confidence and prediction interval bands on the plot and add a legend to differentiate between the lines.
```{r}
ml_hat <- predict(m, newdata=data.frame(brain_size=d$Brain_Size_Species_Mean))
df <- data.frame(cbind(d$Brain_Size_Species_Mean,d$MaxLongevity_m,ml_hat))
names(df) <- c("x","y","yhat")
head(df)
ci <- predict(m,newdata=data.frame(brain_size=d$Brain_Size_Species_Mean), interval="confidence",level=0.90) #90% confidence interval calculation
head(ci)
df <- cbind(df,ci) #adding 90%CI to data frame
names(df) <- c("x","y","yhat","CIfit","CIlwr","CIupr")
head(df)
pi <- predict(m,newdata=data.frame(brain_size=d$Brain_Size_Species_Mean), interval="prediction",level=0.90) #adding prediction intervals
head(pi)
df <- cbind(df,pi)
names(df) <- c("x","y","yhat","CIfit","CIlwr","CIupr","PIfit","PIlwr","PIupr")
head(df)
g <- ggplot(data=df,aes(x=brain_size,y=maximum_longevity)) + geom_point(alpha=1/2)
g <- ggplot(data=df,aes(x=x,y=y))
g <- g + geom_point(alpha=1/2) + geom_line(aes(x=x, y=CIfit, colour = "Best_Fit")) + geom_line(aes(x=x, y=CIlwr, colour = "CI_lower")) + geom_line(aes(x=x, y=CIupr, colour = "CI_upper")) + geom_line(aes(x=x, y=PIlwr, colour = "PI_lower")) + geom_line(aes(x=x, y=PIupr, colour = "PI_upper")) + scale_colour_manual(name="Legend", values=c(Best_Fit="goldenrod", CI_lower="slateblue3", CI_upper="slateblue3", PI_lower = "red2", PI_upper = "red2"))
g

##below is for the log transformed data
log_hat <- predict(logm, newdata=data.frame(lg_brain_size=log(d$Brain_Size_Species_Mean)))
log_hat
lg_df <- data.frame(cbind(log(d$Brain_Size_Species_Mean), log(d$MaxLongevity_m), log_hat))
names(lg_df) <- c("x","y", "yhat")
head(lg_df)
lg_ci <- predict(logm, newdata = data.frame(lg_brain_size=log(d$Brain_Size_Species_Mean)), interval = "confidence", level = 0.90)
head(lg_ci)
lg_df <- cbind(lg_df, lg_ci)
names(lg_df) <- c("x", "y", "yhat", "CIfit", "CIlwr","CIupr")
head(lg_df)
lg_pi <- predict(logm,newdata=data.frame(lg_brain_size=log(d$Brain_Size_Species_Mean)), interval="prediction",level=0.90) #adding prediction intervals
head(lg_pi)
lg_df <- cbind(lg_df,lg_pi)
names(lg_df) <- c("x","y","yhat","CIfit","CIlwr","CIupr","PIfit","PIlwr","PIupr")
head(lg_df)
#putting log transformed data on a plot
logg <- ggplot(data=lg_df,aes(x=lg_brain_size,y=lg_MaxLongevity)) + geom_point(alpha=1/2)
logg <- ggplot(data=lg_df,aes(x=lg_brain_size,y=lg_MaxLongevity))
logg <- logg + geom_point(alpha=1/2) + geom_line(aes(x=x, y=CIfit, colour = "Best_Fit")) + geom_line(aes(x=x, y=CIlwr, colour = "CI_lower")) + geom_line(aes(x=x, y=CIupr, colour = "CI_upper")) + geom_line(aes(x=x, y=PIlwr, colour = "PI_lower")) + geom_line(aes(x=x, y=PIupr, colour = "PI_upper")) + scale_colour_manual(name="Legend", values=c(Best_Fit="magenta", CI_lower="blue3", CI_upper="blue3", PI_lower = "green2", PI_upper = "green2"))
logg
```
*Produce a point estimate and associated 90 percent PI for the longevity of a species whose brain weight is 800 gm. Do you trust the model to predict observations accurately for this value of the explanatory variable? Why or why not?

```{r}
pi <- predict(m,newdata=data.frame(brain_size=800), interval="prediction",level=0.90)
pi
log_pi<- predict(logm,newdata=data.frame(lg_brain_size=log(800)), interval="prediction",level=0.90)
log_pi
##I do not trust the model to predict observations accurately for 800g brain weight.  Our data points are concentrated between 0g and 150g, therefore, the further our x value gets from those weights, the less reliable our model is at predicting the y.  The confidence intervals are too broad at 800g (they span 400 y-values) for the model to be trusted.
```
*Looking at your two models, which do you think is better? Why?
I think that the log transformed model is better because it normalizes the data via making it symmetrical.  Applying a log transformation to the data corrects the heavy left-skew of our data and brings extreme observations closer to a position of centrality.  This makes the data easier to analyze and makes the differences in data points less dramatic.